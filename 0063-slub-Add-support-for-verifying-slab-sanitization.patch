From b86c44df891eb2450f78aca3824c4f7f74a70bad Mon Sep 17 00:00:00 2001
From: Daniel Micay <danielmicay@gmail.com>
Date: Thu, 4 May 2017 15:58:57 -0400
Subject: [PATCH 063/103] slub: Add support for verifying slab sanitization

This is an extension to the sanitization feature in PaX for when
sacricifing more performance for security is acceptable.

The initial version from Daniel Micay was relying on PAGE_SANITIZE. It
now relies on upstream's init_on_free.

Signed-off-by: Daniel Micay <danielmicay@gmail.com>
Signed-off-by: Thibaut Sautereau <thibaut.sautereau@ssi.gouv.fr>
Signed-off-by: Levente Polyak <levente@leventepolyak.net>
[nicolas.bouchinet@ssi.gouv.fr: Should not conflict with commit 520a688a2edfddba9]
Signed-off-by: Nicolas Bouchinet <nicolas.bouchinet@ssi.gouv.fr>
---
 mm/slub.c                  | 45 ++++++++++++++++++++++++++++++++++----
 security/Kconfig.hardening |  8 +++++++
 2 files changed, 49 insertions(+), 4 deletions(-)

diff --git a/mm/slub.c b/mm/slub.c
index 049ffee3a09f..b0a4e85e13b4 100644
--- a/mm/slub.c
+++ b/mm/slub.c
@@ -236,6 +236,12 @@ static inline bool slub_debug_orig_size(struct kmem_cache *s)
 			(s->flags & SLAB_KMALLOC));
 }
 
+static inline bool has_sanitize_verify(struct kmem_cache *s)
+{
+	return IS_ENABLED(CONFIG_SLAB_SANITIZE_VERIFY) &&
+	       slab_want_init_on_free(s);
+}
+
 void *fixup_red_left(struct kmem_cache *s, void *p)
 {
 	if (kmem_cache_debug_flags(s, SLAB_RED_ZONE))
@@ -2257,7 +2263,7 @@ bool slab_free_hook(struct kmem_cache *s, void *x, bool init)
 		 */
 		set_orig_size(s, x, orig_size);
 
-		if (s->ctor)
+		if (!IS_ENABLED(CONFIG_SLAB_SANITIZE_VERIFY) && s->ctor)
 			s->ctor(x);
 	}
 	/* KASAN might put x into memory quarantine, delaying its reuse. */
@@ -2328,7 +2334,7 @@ static void *setup_object(struct kmem_cache *s, void *object)
 {
 	setup_object_debug(s, object);
 	object = kasan_init_slab_obj(s, object);
-	if (unlikely(s->ctor)) {
+	if (unlikely(s->ctor) && !has_sanitize_verify(s)) {
 		kasan_unpoison_new_object(s, object);
 		s->ctor(object);
 		kasan_poison_new_object(s, object);
@@ -4053,7 +4059,19 @@ static __fastpath_inline void *slab_alloc_node(struct kmem_cache *s, struct list
 	object = __slab_alloc_node(s, gfpflags, node, addr, orig_size);
 
 	maybe_wipe_obj_freeptr(s, object);
-	init = slab_want_init_on_alloc(gfpflags, s);
+
+	if (has_sanitize_verify(s) && object) {
+		/* KASAN hasn't unpoisoned the object yet (this is done in the
+		 * post-alloc hook), so let's do it temporarily.
+		 */
+		kasan_unpoison_new_object(s, object);
+		BUG_ON(memchr_inv(object, 0, s->object_size));
+		if (s->ctor)
+			s->ctor(object);
+		kasan_poison_new_object(s, object);
+	} else {
+		init = slab_want_init_on_alloc(gfpflags, s);
+	}
 
 out:
 	/*
@@ -4817,6 +4835,21 @@ int __kmem_cache_alloc_bulk(struct kmem_cache *s, gfp_t flags, size_t size,
 	local_unlock_irqrestore(&s->cpu_slab->lock, irqflags);
 	slub_put_cpu_ptr(s->cpu_slab);
 
+	if (has_sanitize_verify(s)) {
+		int j;
+
+		for (j = 0; j < i; j++) {
+			/* KASAN hasn't unpoisoned the object yet (this is done in the
+			 * post-alloc hook), so let's do it temporarily.
+			 */
+			kasan_unpoison_new_object(s, p[j]);
+			BUG_ON(memchr_inv(p[j], 0, s->object_size));
+			if (s->ctor)
+				s->ctor(p[j]);
+			kasan_poison_new_object(s, p[j]);
+		}
+	}
+
 	return i;
 
 error:
@@ -4860,6 +4893,7 @@ int kmem_cache_alloc_bulk_noprof(struct kmem_cache *s, gfp_t flags, size_t size,
 				 void **p)
 {
 	int i;
+	bool init = false;
 
 	if (!size)
 		return 0;
@@ -4876,8 +4910,11 @@ int kmem_cache_alloc_bulk_noprof(struct kmem_cache *s, gfp_t flags, size_t size,
 	 * memcg and kmem_cache debug support and memory initialization.
 	 * Done outside of the IRQ disabled fastpath loop.
 	 */
+	if (!has_sanitize_verify(s)) {
+		init = slab_want_init_on_alloc(flags, s);
+	}
 	if (unlikely(!slab_post_alloc_hook(s, NULL, flags, size, p,
-		    slab_want_init_on_alloc(flags, s), s->object_size))) {
+		    init, s->object_size))) {
 		return 0;
 	}
 	return i;
diff --git a/security/Kconfig.hardening b/security/Kconfig.hardening
index c017b5bfa9bc..29552add6301 100644
--- a/security/Kconfig.hardening
+++ b/security/Kconfig.hardening
@@ -286,6 +286,14 @@ config PAGE_SANITIZE_VERIFY
 	  When init_on_free is enabled, verify that newly allocated pages
 	  are zeroed to detect write-after-free bugs.
 
+config SLAB_SANITIZE_VERIFY
+	bool "Verify sanitized SLAB allocations"
+	default y
+	depends on !KASAN
+	help
+	  When init_on_free is enabled, verify that newly allocated slab
+	  objects are zeroed to detect write-after-free bugs.
+
 endmenu
 
 menu "Hardening of kernel data structures"
-- 
2.47.0

